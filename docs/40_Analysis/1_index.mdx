# Analysis

* Diagrams of tokens that are generated from various analyzers
* Built-in analyzers from lucene.keyword to the others
* Labs: lucene.standard, lucene.english, custom analyzer

https://www.mongodb.com/docs/atlas/atlas-search/analyzers/

![Visual Editor standard analyzer output](/img/editor_analysis.png)

## Analysis matters
“searches” query does not match “Search” with lucene.standard: https://search-playground.mongodb.com/tools/code-playground/snapshots/664739964e0a3f240a5de9db
“searches” matches “Search” using lucene.english: https://search-playground.mongodb.com/tools/code-playground/snapshots/66473aa64e0a3f240a5de9dd

## Custom analyzers
Last 4 digit of phone number matching (regex extraction during indexing, keyword analysis at query time): https://search-playground.corp.mongodb.com/tools/code-playground/snapshots/669e6c98d49ef6fad98118ba
Example of being able to do ‘startsWith’ and ‘endsWith’ using wildcard and ‘reverse’ token filter:
https://search-playground.mongodb.com/tools/code-playground/snapshots/6683c8bc4a45448733549bbc

Example of being able to do ‘startsWith’, ‘endsWith’ and ‘contains’ using nGrams: https://search-playground.mongodb.com/tools/code-playground/snapshots/6683c999934a05d9b585b6e7 
Relevancy
Example of an as-you-type suggest configuration; sophisticated use of multi and several weighted query clauses: https://search-playground.mongodb.com/tools/code-playground/snapshots/66473b744e0a3f240a5de9e1
$project score?
$project scoreDetails?

# multi
Why?
Relevancy example: boost each multi uniquely
Multiple language example: may not know the language of the content and each document could be different - multi across all possible languages, query across them as desired at query-time, let relevancy sort it out
Example of being able to do ‘startsWith’ and ‘endsWith’ using wildcard and ‘reverse’ token filter:
https://search-playground.mongodb.com/tools/code-playground/snapshots/6683c8bc4a45448733549bbc


* Text: the heart and soul of your content
* Strings are analyzed, tokenized into terms
  * Multiple analyzers can be used for a single string field (multi)
* Terms: words, fragments, atomic searchable units
* An inverted index structure organizes terms lexicographically/alphabetically for quick lookup (aka a dictionary)
* Term statistics:
  * Posting list: document identifiers
  * Term frequency (tf): how many occurrences of the term per document
  * Document frequency (df): how many documents contain the term
  * Positions: where in the document does this term occur

lucene.standard (default): tokenizes at word break characters, removes punctuation, and lowercases
lucene.english: standard tokenization plus de-pluralization, stop word removal, and stemming
lucene.keyword: Tokenizes text as a single term; suitable for wildcard or regex matching over entire value
Many language-specific analyzers built-in: (lucene.)arabic, armenian, basque, bengali, brazilian, bulgarian, catalan, chinese, cjk, czech, danish, dutch, english, finnish, french, galician, german, greek, hindi, hungarian, indonesian, irish, italian, japanese, korean, kuromoji, latvian, lithuanian, morfologik, nori, norwegian, persian, polish, portuguese, romanian, russian, smartcn, sorani, spanish, swedish, thai, turkish, ukrainian

`searchAnalyzer` vs. `analyzer`

